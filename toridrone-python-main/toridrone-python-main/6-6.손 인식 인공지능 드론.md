# 손 인식하기
* MediaPipe를 사용해서 손가락을 인식할 수 있습니다.
* MediaPipe의 Hands 모델은 손가락과 손의 위치를 알려줍니다.
* 손가락을 그림과 같이 점으로 나타냅니다.   
![image](https://github.com/user-attachments/assets/80c9b1d9-dfb4-450a-a5c8-713028139eda)
```python
import cv2
import mediapipe as mp

capture = cv2.VideoCapture(0)

# 손 인식하는 모듈입니다.
mp_hands = mp.solutions.hands

# 인식한 손을 그리는 모듈입니다.
mp_drawing = mp.solutions.drawing_utils

# mp_hands.Hands로 손가락을 인식하는 모듈을 초기화합니다.
# max_num_hands에 인식할 손의 개수를 정합니다.
# min_detection_confidence와 min_tracking_confidence는 옵션입니다. 둘 다 0.5로 설정합니다.
with mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
    ) as hands:

    while capture.isOpened():
        ret, frame = capture.read()
        if not ret:
            continue
        
        # 좌우 반전하고 BRG 색상을 RGB 색상으로 바꿉니다. 
        frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)

        # 손을 인식합니다.
        results = hands.process(frame)

        # RGB 색상을 BGR 색상으로 바꿉니다.
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

        # 손을 인식했다면
        if results.multi_hand_landmarks:
            # results.multi_hand_landmarks에는 mp_hands.Hands로 정한 손의 수만큼 리스트로 값이 저장되어 있습니다.
            for hand_landmarks in results.multi_hand_landmarks:  
                # 인식한 손을 그립니다.                          
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
        cv2.imshow('MediaPipe Hand Detection', frame)
        if cv2.waitKey(1) == 27:
            break
        
capture.release()
cv2.destroyAllWindows()
```
* 손가락 마디마다 지정된 번호가 있습니다. hand_landmarks.landmark[번호]로 좌표를 확인할 수 있습니다.
* 엄지[4번]와 검지[8번]으로 드론을 조종하겠습니다.
* 검지 손가락의 좌푯값을 읽고 화면 왼쪽 위에 그 값을 표시합니다.
```python
import cv2
import mediapipe as mp

capture = cv2.VideoCapture(0)
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

with mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
    ) as hands:

    while capture.isOpened():
        ret, frame = capture.read()
        if not ret:
            continue 
        frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)
        results = hands.process(frame)
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        if results.multi_hand_landmarks:            
            for hand_landmarks in results.multi_hand_landmarks: 
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS) 
                
                # 검지 손가락을 선택합니다.
                finger = hand_landmarks.landmark[8] 

                # 검지 손가락의 좌표로 텍스트를 만듭니다.
                # finger.x는 x좌표, finger.y는 y좌표입니다.
                # 소수점 둘째 자리까지 나타냅니다.
                text = 'x : {:.2f}, y : {:.2f}'.format(finger.x, finger.y)
                cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 0, 0), 1)                
        cv2.imshow('MediaPipe Hand Detection', frame)
        if cv2.waitKey(1) == 27:
            break
        
capture.release()
cv2.destroyAllWindows()
```

# 손 인식 인공지능 드론 만들기
* 검지 손가락으로 롤과 피치를 바꿉니다. 코의 좌표로 드론을 움직인 것처럼 값을 정합니다.
* 검지 손가락 좌푯값을 -100과 100사이로 바꿔서 쓰로틀(Throttle), 요우(Yaw), 피치(Pitch), 롤(Roll) 값을 정합니다.
* 드론이 너무 빨리 움직이지 않도록 0.5를 곱합니다.
```python
import cv2
import mediapipe as mp
from time import sleep
import keyboard
from CodingRider.drone import *
from CodingRider.protocol import *

# is_takeoff 변수로 이륙했는지 확인합니다.
is_takeoff = False

drone = Drone()
drone.open('COM3')

capture = cv2.VideoCapture(0)
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

with mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
    ) as hands:

    while capture.isOpened():
        ret, frame = capture.read()
        if not ret:
            continue 
        frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)
        results = hands.process(frame)
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        if results.multi_hand_landmarks:            
            for hand_landmarks in results.multi_hand_landmarks: 
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS) 
                finger = hand_landmarks.landmark[8] 
                # 이륙했다면 검지 손가락 좌표로 드론을 움직입니다.   
                if is_takeoff:                
                    # 롤과 피치 값을 정합니다.
                    roll = int(((finger.x * 200) - 100) * 0.5)
                    pitch = -int(((finger.y * 200) - 100) * 0.5)
                    drone.sendControl(roll, pitch, 0, 0)

                    # 롤과 피치 값을 나타냅니다.    
                    text = f'r : {roll}, p : {pitch}'
                    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 0, 0), 1)                
        cv2.imshow('MediaPipe Hand Detection', frame)
        if keyboard.is_pressed('enter'):   
            print('이륙')       
            sleep(2)
            drone.sendTakeOff()
            sleep(5)
            print('이륙 완료') 
            is_takeoff = True
        if keyboard.is_pressed('space'):
            print('착륙')
            drone.sendControlWhile(0, 0, 0, 0, 500) # 제자리에서 0.5초 멈춥니다.
            drone.sendLanding()
            sleep(3)
            print('착륙 완료')
            is_takeoff = False
        if keyboard.is_pressed('q'):
            print('정지')
            drone.sendControlWhile(0, 0, 0, 0, 500)
            drone.sendStop()
            sleep(3)
            is_takeoff = False
        if cv2.waitKey(1) == 27:
            break
        
capture.release()
cv2.destroyAllWindows()
```
