# MediaPipe
* MediaPipe는 구글에서 개발한 오픈 소스 플랫폼 프레임워크로 얼굴인식, 포즈, 객체감지, 모션트레킹 등 다양한 형태의 기능과 모델을 제공합니다.
* MediaPipe 사이트에서 다양한 예제를 확인할 수 있습니다.
  * https://ai.google.dev/edge/mediapipe/solutions
* MediaPipe 라이브러리는 현재 파이썬 3.8~3.11 버전에서 사용할 수 있습니다.
* 나중에 사용할 수 있는 버전은 바뀔 수 있습니다. 아래 링크에서 사용할 수 있는 파이썬 버전을 확인해서 사용합니다.
  * https://developers.google.com/mediapipe/solutions/setup_python
* ```pip install mediapipe```
* user 관련 문제로 설치가 잘 안 되면 ```pip install --user mediapipe```로 설치합니다.

# 좌표 확인하기
* 얼굴을 촬영해서 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀 위치를 확인하는 방법을 알아보겠습니다.
* 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀 위치가 점을 표시합니다.
* print(detection)으로 어떤 내용인지 확인합니다.
* print(detection)를 보면 6개의 relative_keypoints가 있습니다.
* 각각 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀 좌푯값입니다.
```python
import cv2
import mediapipe as mp

capture = cv2.VideoCapture(0)
                       
# 얼굴을 찾고, 찾은 얼굴에 표시를 해주기 위한 변수를 정의합니다.
# 얼굴 검출을 위해서 face_detection 모듈을 사용합니다.
mp_face_detection = mp.solutions.face_detection 

# 얼굴의 특징을 그리기 위해서 drawing_utils 모듈을 사용합니다.
mp_drawing = mp.solutions.drawing_utils 

# mediapipe 사용방법에 맞게 설정합니다.
# model_selection = 0은 카메라와 가까운 것을 인식합니다.
# model_selection = 1은 카메라와 먼 것을 인식합니다.
# min_detection_confidence=0.7는 70% 확신한다면 얼굴로 본다는 뜻입니다.
with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
    while capture.isOpened():
        ret, frame = capture.read()
        if not ret:
            continue     

        # 성능을 높이기 위해 프레임을 쓰기 불가능 상태로 변경합니다.        
        frame.flags.writeable = False
        
        # 좌우 반전을 합니다.
        frame = cv2.flip(frame, 1) 

        # BGR 색상을 RGB 색상으로 바꿉니다.
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # frame에서 얼굴을 확인합니다. 
        results = face_detection.process(frame) 

        # 다시 프레임을 쓰기 가능 상태로 바꿉니다.      
        frame.flags.writeable = True

        # RGB 색상을 BGR 색상으로 바꿉니다
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  

        # 얼굴을 인식했다면      
        if results.detections:       
            for detection in results.detections:
                # 그 결과를 frame에 그립니다. 
                # 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀 위치가 점을 표시됩니다.
                mp_drawing.draw_detection(frame, detection) 

                # 얼굴의 정보를 출력합니다. 
                print(detection)
                
        cv2.imshow('MediaPipe Face Detection', frame)
        if cv2.waitKey(1) == 27:
            break
            
capture.release()
cv2.destroyAllWindows()
```
* relative_keypoints에서 코의 좌푯값을 읽고 화면 왼쪽 위에 그 값을 표시합니다.
* 코 위치를 원으로 그립니다.
```python
import cv2
import mediapipe as mp
from time import sleep
import keyboard
from CodingRider.drone import *
from CodingRider.protocol import *

# is_takeoff 변수로 이륙했는지 확인합니다.
is_takeoff = False

drone = Drone()
drone.open('COM3')

capture = cv2.VideoCapture(0)
mp_face_detection = mp.solutions.face_detection 
mp_drawing = mp.solutions.drawing_utils 

with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
    while capture.isOpened():
        ret, frame = capture.read()
        if not ret:
            continue
        frame.flags.writeable = False
        frame = cv2.flip(frame, 1) 
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)       
        results = face_detection.process(frame)         
        frame.flags.writeable = True   
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)              
        if results.detections:          
            for detection in results.detections:                
                # 위치를 나타내는 것을 keypoints 리스트에 저장합니다.
                keypoints = detection.location_data.relative_keypoints 

                # keypoints[2]은 코입니다.
                # keypoints[2].x는 x좌표이고 keypoints[2].y는 y좌표입니다.  
                nose = keypoints[2]              

                # frame의 크기로 계산합니다.
                # frame.shape은 세로, 가로, 채널 값입니다. 
                height, width, channel = frame.shape

                # 얼굴의 각 부위의 좌표값은 0에서 1사이로 정해집니다. 
                # 여기에 frame의 가로와 세로 크기를 곱해서 실제 좌표값을 구합니다.
                nose_position = (int(nose.x * width), int(nose.y * height))
                 
                # 코의 좌푯값을 나타냅니다.
                text = f'x : {nose_position[0]}, y : {nose_position[1]}'
                cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 0, 0), 1)

                # 코에 원을 그립니다.       
                cv2.circle(frame, nose_position, 50, (0,0,255), 10, cv2.LINE_AA)       
        cv2.imshow('MediaPipe Face Detection', frame)
        if cv2.waitKey(1) == 27:
            break
            
capture.release()
cv2.destroyAllWindows()
```

# 얼굴 인식 인공지능 드론 만들기
* 코의 좌표로 드론을 움직입니다.
* 키보드로 드론을 이륙/착륙합니다.
* 이륙을 했다면 코의 좌표로 드론을 움직입니다.
* 마우스로 드론을 움직였던 것처럼 좌푯값을 -100과 100사이로 바꿔서 쓰로틀(Throttle), 요우(Yaw), 피치(Pitch), 롤(Roll) 값을 정합니다.
* 드론이 너무 빨리 움직이지 않도록 0.5를 곱합니다.
```python
roll = int(((nose.x * 200) - 100) * 0.5)
pitch = -int(((nose.y * 200) - 100) * 0.5)
```
```python
import cv2
import mediapipe as mp
from time import sleep
import keyboard
from CodingRider.drone import *
from CodingRider.protocol import *

# is_takeoff 변수로 이륙했는지 확인합니다.
is_takeoff = False

# 드론 객체를 만듭니다.
drone = Drone()
drone.open('COM3')

capture = cv2.VideoCapture(0)
mp_face_detection = mp.solutions.face_detection 
mp_drawing = mp.solutions.drawing_utils 

with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
    while capture.isOpened():
        ret, frame = capture.read()
        if not ret:
            continue
        frame.flags.writeable = False
        frame = cv2.flip(frame, 1) 
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)       
        results = face_detection.process(frame)         
        frame.flags.writeable = True   
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)              
        if results.detections:          
            for detection in results.detections:
                keypoints = detection.location_data.relative_keypoints 
                nose = keypoints[2] 
                height, width, channel = frame.shape
                nose_position = (int(nose.x * width), int(nose.y * height))               
                cv2.circle(frame, nose_position, 50, (0,0,255), 10, cv2.LINE_AA)
        if keyboard.is_pressed('enter'):
            print('이륙')
            sleep(2)
            drone.sendTakeOff()
            sleep(5)
            print('이륙 완료')
            is_takeoff = True               
        if keyboard.is_pressed('space'):
            print('착륙')
            drone.sendControlWhile(0, 0, 0, 0, 500) 
            drone.sendLanding()
            sleep(3)
            print('착륙 완료')
            is_takeoff = False
        if keyboard.is_pressed('q'):
            print('정지')
            drone.sendControlWhile(0, 0, 0, 0, 500)
            drone.sendStop()
            sleep(3)   
            is_takeoff = False
        # 이륙했다면 코의 좌표로 드론을 움직입니다.   
        if is_takeoff: 
            # 롤과 피치 값을 정합니다.          
            roll = int(((nose.x * 200) - 100) * 0.5)
            pitch = -int(((nose.y * 200) - 100) * 0.5)
            drone.sendControl(roll, pitch, 0, 0)

            # 롤과 피치 값을 나타냅니다.
            text = f'r : {roll}, p : {pitch}'
            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 0, 0), 1)
        cv2.imshow('MediaPipe Face Detection', frame)
        if cv2.waitKey(1) == 27:
            break  
                  
capture.release()
cv2.destroyAllWindows()
```
